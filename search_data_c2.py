# ç¡®ä¿å·²å®‰è£…å¿…è¦çš„åº“ 
# pip install -U jarvis-tools pandas matplotlib seaborn numpy

from jarvis.db.figshare import data
import pandas as pd
import numpy as np
import pickle
import os
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class OptimizedJarvisConfig:
    """åŸºäºå®é™…æ•°æ®åˆ†æä¼˜åŒ–çš„é…ç½®ç®¡ç†ç±»"""
    
    # ğŸ† æœ€ç»ˆæ¨èç»„åˆï¼šç”µå­æ€§è´¨æ ¸å¿ƒåŒ… + å½¢æˆèƒ½è¾…åŠ©ä»»åŠ¡
    FINAL_MTL_BUNDLE = [
        'optb88vdw_bandgap',              # PBEå¸¦éš™ï¼ˆåŸºç¡€ï¼Œæ•°æ®æœ€å…¨ï¼‰
        'dfpt_piezo_max_dielectric',      # ä»‹ç”µå¸¸æ•°ï¼ˆæ ¸å¿ƒç”µå­æ€§è´¨ï¼‰
        'avg_elec_mass',                  # ç”µå­æœ‰æ•ˆè´¨é‡
        'avg_hole_mass',                  # ç©ºç©´æœ‰æ•ˆè´¨é‡
        'formation_energy_peratom'        # å½¢æˆèƒ½ï¼ˆå¼ºè¾…åŠ©ä»»åŠ¡ï¼Œç¨³å®šè®­ç»ƒï¼‰
    ]
    
    # å¤‡é€‰ç»„åˆï¼ˆç”¨äºå¯¹æ¯”å®éªŒï¼‰
    BASIC_COMPLETE_BUNDLE = [
        'formation_energy_peratom',
        'optb88vdw_bandgap', 
        'optb88vdw_total_energy',
        'density'
    ]
    
    # çº¯ç”µå­æ€§è´¨ç»„åˆï¼ˆé«˜ç§‘å­¦ä»·å€¼ï¼‰
    PURE_ELECTRONIC_BUNDLE = [
        'optb88vdw_bandgap',
        'mbj_bandgap',
        'hse_gap',
        'avg_elec_mass',
        'avg_hole_mass'
    ]

class JarvisDataHandler:
    """
    é’ˆå¯¹GCPNet-Plusä¼˜åŒ–çš„JARVISæ•°æ®å¤„ç†å™¨
    """
    
    def __init__(self, cache_dir='jarvis_cache'):
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
        self.cache_dir = cache_dir
        self.cache_file = os.path.join(cache_dir, 'dft_3d_2021_full.pkl')
        self.metadata_file = os.path.join(cache_dir, 'dft_3d_2021_metadata.pkl')

    def _save_cache(self, df):
        """ä¿å­˜å®Œæ•´æ•°æ®é›†çš„ç¼“å­˜"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(df, f)
            metadata = {'download_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            with open(self.metadata_file, 'wb') as f:
                pickle.dump(metadata, f)
            print(f"åŸå§‹æ•°æ®å·²ç¼“å­˜åˆ° {self.cache_file}")
        except Exception as e:
            print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        if os.path.exists(self.cache_file):
            print(f"ä»ç¼“å­˜æ–‡ä»¶ {self.cache_file} åŠ è½½...")
            try:
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        return None

    def download_jarvis_data(self, use_cache=True):
        """ä¸‹è½½æˆ–ä»ç¼“å­˜åŠ è½½å®Œæ•´çš„dft_3d_2021æ•°æ®é›†"""
        if use_cache:
            df = self._load_cache()
            if df is not None:
                if len(df) == 0:
                    print("è­¦å‘Šï¼šç¼“å­˜æ•°æ®ä¸ºç©ºï¼Œé‡æ–°ä¸‹è½½...")
                    return self.download_jarvis_data(use_cache=False)
                return df
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"ä¸‹è½½å°è¯• {attempt + 1}/{max_retries}...")
                full_data = data('dft_3d_2021')
                df = pd.DataFrame(full_data)
                
                if len(df) == 0:
                    raise ValueError("ä¸‹è½½çš„æ•°æ®é›†ä¸ºç©º")
                
                self._save_cache(df)
                print(f"âœ“ ä¸‹è½½æˆåŠŸï¼å…±è·å– {len(df)} æ¡ææ–™è®°å½•ã€‚")
                return df
            except Exception as e:
                print(f"âœ— ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt == max_retries - 1:
                    print(f"âœ— æ‰€æœ‰ä¸‹è½½å°è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
                    return None
        return None

    def clean_and_convert_data_types(self, df):
        """æ¸…ç†å’Œè½¬æ¢æ•°æ®ç±»å‹"""
        print(f"\nğŸ”§ æ¸…ç†å’Œè½¬æ¢æ•°æ®ç±»å‹...")
        
        # è¯†åˆ«åº”è¯¥æ˜¯æ•°å€¼çš„åˆ—
        potential_numeric_cols = []
        for col in df.columns:
            if any(keyword in col.lower() for keyword in [
                'bandgap', 'energy', 'mass', 'density', 'modulus', 'gap',
                'dielectric', 'piezo', 'seebeck', 'kappa', 'formation',
                'total', 'bulk', 'shear', 'poisson', 'ehull'
            ]):
                potential_numeric_cols.append(col)
        
        conversion_report = []
        
        for col in potential_numeric_cols:
            if col in df.columns:
                original_type = str(df[col].dtype)
                original_non_null = df[col].notna().sum()
                
                try:
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    new_non_null = df[col].notna().sum()
                    converted_successfully = new_non_null > 0
                    
                    conversion_report.append({
                        'column': col,
                        'original_type': original_type,
                        'final_type': str(df[col].dtype),
                        'original_valid': original_non_null,
                        'final_valid': new_non_null,
                        'conversion_success': converted_successfully,
                        'data_loss': original_non_null - new_non_null
                    })
                    
                    if converted_successfully:
                        if new_non_null < original_non_null:
                            print(f"âœ“ {col}: {original_type} -> {df[col].dtype} (æŸå¤± {original_non_null - new_non_null} ä¸ªå€¼)")
                        else:
                            print(f"âœ“ {col}: {original_type} -> {df[col].dtype}")
                    else:
                        print(f"âš ï¸ {col}: è½¬æ¢å¤±è´¥ï¼Œæ‰€æœ‰å€¼å˜ä¸ºNaN")
                        
                except Exception as e:
                    print(f"âœ— {col}: è½¬æ¢å¤±è´¥ - {e}")
                    conversion_report.append({
                        'column': col,
                        'original_type': original_type,
                        'final_type': original_type,
                        'original_valid': original_non_null,
                        'final_valid': original_non_null,
                        'conversion_success': False,
                        'data_loss': 0
                    })
        
        # ä¿å­˜è½¬æ¢æŠ¥å‘Š
        if conversion_report:
            report_df = pd.DataFrame(conversion_report)
            report_file = os.path.join(self.cache_dir, 'data_type_conversion_report.csv')
            report_df.to_csv(report_file, index=False)
            print(f"\næ•°æ®ç±»å‹è½¬æ¢æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return df

    def explore_column_types(self, df):
        """æ¢ç´¢æ‰€æœ‰åˆ—çš„æ•°æ®ç±»å‹å’Œå†…å®¹"""
        print(f"\nğŸ” æ¢ç´¢æ•°æ®åˆ—ç±»å‹å’Œå†…å®¹...")
        
        type_analysis = []
        for col in df.columns:
            dtype = str(df[col].dtype)
            non_null_count = df[col].notna().sum()
            null_count = df[col].isna().sum()
            
            # æ£€æŸ¥æ•°æ®å†…å®¹ç¤ºä¾‹
            sample_values = df[col].dropna().head(3).tolist()
            sample_str = str(sample_values)[:50] + "..." if len(str(sample_values)) > 50 else str(sample_values)
            
            type_analysis.append({
                'column': col,
                'dtype': dtype,
                'non_null_count': non_null_count,
                'null_count': null_count,
                'sample_values': sample_str
            })
        
        analysis_df = pd.DataFrame(type_analysis)
        
        # ä¿å­˜ç±»å‹åˆ†æ
        type_file = os.path.join(self.cache_dir, 'column_type_analysis.csv')
        analysis_df.to_csv(type_file, index=False)
        print(f"åˆ—ç±»å‹åˆ†æå·²ä¿å­˜åˆ°: {type_file}")
        
        # æ˜¾ç¤ºæ½œåœ¨çš„é—®é¢˜åˆ—
        print(f"\nâš ï¸ éœ€è¦ç‰¹åˆ«æ³¨æ„çš„åˆ—:")
        problematic_cols = analysis_df[analysis_df['dtype'] == 'object']
        for _, row in problematic_cols.iterrows():
            print(f"  {row['column']} (objectç±»å‹): {row['sample_values']}")
        
        return analysis_df

    def safe_numeric_stats(self, series):
        """å®‰å…¨åœ°è®¡ç®—æ•°å€¼ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if pd.api.types.is_numeric_dtype(series):
                valid_data = series.dropna()
                if len(valid_data) > 0:
                    return {
                        'mean': float(valid_data.mean()),
                        'std': float(valid_data.std()),
                        'min': float(valid_data.min()),
                        'max': float(valid_data.max())
                    }
            return {'mean': np.nan, 'std': np.nan, 'min': np.nan, 'max': np.nan}
        except Exception as e:
            print(f"è®¡ç®—ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return {'mean': np.nan, 'std': np.nan, 'min': np.nan, 'max': np.nan}

    def validate_property_bundle(self, df, property_bundle, bundle_name=""):
        """éªŒè¯å±æ€§ç»„åˆçš„æœ‰æ•ˆæ€§å¹¶æä¾›è¯¦ç»†ç»Ÿè®¡"""
        print(f"\n{'='*60}")
        print(f"éªŒè¯å±æ€§ç»„åˆ{bundle_name}: {property_bundle}")
        print(f"{'='*60}")
        
        available_cols = df.columns.tolist()
        validation_results = []
        
        for prop in property_bundle:
            if prop in available_cols:
                valid_count = df[prop].notna().sum()
                valid_ratio = valid_count / len(df) * 100
                
                # å®‰å…¨åœ°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                stats = self.safe_numeric_stats(df[prop])
                
                # ä¿®å¤æ ¼å¼åŒ–é—®é¢˜ï¼šå°†dtypeè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                dtype_str = str(df[prop].dtype)
                
                validation_results.append({
                    'property': prop,
                    'status': 'âœ“ å¯ç”¨',
                    'dtype': dtype_str,
                    'valid_count': valid_count,
                    'valid_ratio': f'{valid_ratio:.1f}%',
                    'mean': f'{stats["mean"]:.4f}' if not np.isnan(stats["mean"]) else 'N/A',
                    'std': f'{stats["std"]:.4f}' if not np.isnan(stats["std"]) else 'N/A'
                })
                
                # æ£€æŸ¥æ•°æ®ç±»å‹ - ä¿®å¤æ ¼å¼åŒ–é—®é¢˜
                if pd.api.types.is_numeric_dtype(df[prop]):
                    print(f"âœ“ {prop:<35} | ç±»å‹: {dtype_str:<10} | æœ‰æ•ˆæ•°æ®: {valid_count:>6} ({valid_ratio:>5.1f}%)")
                else:
                    print(f"âš ï¸ {prop:<35} | ç±»å‹: {dtype_str:<10} | æœ‰æ•ˆæ•°æ®: {valid_count:>6} ({valid_ratio:>5.1f}%) [éæ•°å€¼ç±»å‹]")
                    
            else:
                validation_results.append({
                    'property': prop,
                    'status': 'âœ— ä¸å¯ç”¨',
                    'dtype': 'N/A',
                    'valid_count': 0,
                    'valid_ratio': '0.0%',
                    'mean': 'N/A',
                    'std': 'N/A'
                })
                print(f"âœ— {prop:<35} | ä¸å­˜åœ¨äºæ•°æ®é›†ä¸­")
        
        # ä¿å­˜éªŒè¯ç»“æœ
        results_df = pd.DataFrame(validation_results)
        validation_file = os.path.join(self.cache_dir, f'validation_{bundle_name.lower().replace(" ", "_")}.csv')
        results_df.to_csv(validation_file, index=False)
        print(f"\néªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {validation_file}")
        
        valid_props = [r['property'] for r in validation_results if 'âœ“' in r['status']]
        invalid_props = [r['property'] for r in validation_results if 'âœ—' in r['status']]
        
        # æ£€æŸ¥æ•°å€¼ç±»å‹çš„æœ‰æ•ˆå±æ€§
        numeric_valid_props = []
        for prop in valid_props:
            if prop in df.columns and pd.api.types.is_numeric_dtype(df[prop]):
                numeric_valid_props.append(prop)
        
        if len(numeric_valid_props) != len(valid_props):
            print(f"\nâš ï¸ æ³¨æ„: {len(valid_props) - len(numeric_valid_props)} ä¸ªå±æ€§ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œå°†åœ¨åç»­åˆ†æä¸­è·³è¿‡")
        
        return numeric_valid_props, invalid_props, validation_results

    def analyze_data_intersection(self, df, property_bundle):
        """åˆ†æå¤šä¸ªæ€§è´¨çš„æ•°æ®äº¤é›†ï¼Œè¿™å¯¹MTLè‡³å…³é‡è¦"""
        print(f"\nåˆ†æ {len(property_bundle)} ä¸ªæ€§è´¨çš„æ•°æ®äº¤é›†:")
        
        # åªä½¿ç”¨æ•°å€¼ç±»å‹çš„åˆ—
        numeric_props = [prop for prop in property_bundle 
                        if prop in df.columns and pd.api.types.is_numeric_dtype(df[prop])]
        
        if len(numeric_props) != len(property_bundle):
            excluded = set(property_bundle) - set(numeric_props)
            print(f"âš ï¸ æ’é™¤éæ•°å€¼å±æ€§: {excluded}")
        
        # é€æ­¥åˆ†ææ•°æ®äº¤é›†
        intersection_analysis = []
        current_data = df.copy()
        
        for i, prop in enumerate(numeric_props):
            before_count = len(current_data)
            current_data = current_data.dropna(subset=[prop])
            after_count = len(current_data)
            lost_count = before_count - after_count
            
            intersection_analysis.append({
                'step': i + 1,
                'added_property': prop,
                'remaining_samples': after_count,
                'lost_samples': lost_count,
                'loss_rate': f'{lost_count/before_count*100:.1f}%' if before_count > 0 else '0.0%'
            })
            
            print(f"Step {i+1}: åŠ å…¥ {prop}")
            print(f"  å‰©ä½™æ ·æœ¬: {after_count:,} (æŸå¤±: {lost_count:,}, {lost_count/before_count*100:.1f}%)")
        
        # ä¿å­˜äº¤é›†åˆ†æç»“æœ
        intersection_df = pd.DataFrame(intersection_analysis)
        intersection_file = os.path.join(self.cache_dir, 'data_intersection_analysis.csv')
        intersection_df.to_csv(intersection_file, index=False)
        print(f"\næ•°æ®äº¤é›†åˆ†æå·²ä¿å­˜åˆ°: {intersection_file}")
        
        final_count = len(current_data)
        original_count = len(df)
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   åŸå§‹æ•°æ®é›†: {original_count:,} æ ·æœ¬")
        print(f"   äº¤é›†æ•°æ®é›†: {final_count:,} æ ·æœ¬")
        print(f"   ä¿ç•™ç‡: {final_count/original_count*100:.1f}%")
        
        return current_data, intersection_analysis, numeric_props

    def plot_enhanced_correlation_matrix(self, df, property_bundle, bundle_name="", save_plot=True):
        """ç»˜åˆ¶å¢å¼ºç‰ˆç›¸å…³æ€§çŸ©é˜µ"""
        print(f"\nä¸º {bundle_name} ç”Ÿæˆç›¸å…³æ€§çŸ©é˜µ...")
        
        # ç¡®ä¿æ‰€æœ‰å±æ€§éƒ½æ˜¯æ•°å€¼ç±»å‹
        numeric_props = [prop for prop in property_bundle 
                        if prop in df.columns and pd.api.types.is_numeric_dtype(df[prop])]
        
        if len(numeric_props) != len(property_bundle):
            excluded = set(property_bundle) - set(numeric_props)
            print(f"âš ï¸ æ’é™¤éæ•°å€¼å±æ€§: {excluded}")
        
        if len(numeric_props) < 2:
            print(f"âŒ æ•°å€¼å±æ€§å°‘äº2ä¸ªï¼Œæ— æ³•ç”Ÿæˆç›¸å…³æ€§çŸ©é˜µ")
            return None, None
        
        # åªä½¿ç”¨æœ‰å®Œæ•´æ•°æ®çš„æ ·æœ¬
        clean_data = df[numeric_props].dropna()
        if len(clean_data) < 100:
            print(f"è­¦å‘Šï¼šå¯ç”¨æ ·æœ¬è¿‡å°‘ (n={len(clean_data)})ï¼Œå»ºè®®è‡³å°‘100ä¸ªæ ·æœ¬")
            if len(clean_data) < 10:
                return None, None
        
        corr_data = clean_data.corr()
        
        # åˆ›å»ºæ›´ç¾è§‚çš„å›¾è¡¨
        plt.figure(figsize=(14, 12))
        
        # åˆ›å»ºmaskåªæ˜¾ç¤ºä¸‹ä¸‰è§’
        mask = np.triu(np.ones_like(corr_data, dtype=bool))
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(corr_data, 
                    mask=mask, 
                    annot=True, 
                    cmap='RdBu_r', 
                    center=0, 
                    fmt='.3f', 
                    square=True,
                    cbar_kws={"shrink": .8, "label": "Pearson Correlation"},
                    annot_kws={"size": 12})
        
        plt.title(f'{bundle_name} æ€§è´¨ç›¸å…³æ€§çŸ©é˜µ\n(æ ·æœ¬æ•°: {len(clean_data):,})', 
                  fontsize=16, pad=20)
        plt.xlabel('æ€§è´¨', fontsize=14)
        plt.ylabel('æ€§è´¨', fontsize=14)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        if save_plot:
            plot_file = os.path.join(self.cache_dir, f'correlation_matrix_{bundle_name.lower().replace(" ", "_")}.png')
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            print(f"ç›¸å…³æ€§å›¾å·²ä¿å­˜åˆ°: {plot_file}")
        
        plt.show()
        
        # è¾“å‡ºå…³é”®ç›¸å…³æ€§ç»Ÿè®¡
        print(f"\nç›¸å…³æ€§ç»Ÿè®¡æ‘˜è¦:")
        correlations = []
        for i in range(len(numeric_props)):
            for j in range(i+1, len(numeric_props)):
                corr_val = corr_data.iloc[i, j]
                correlations.append({
                    'property_1': numeric_props[i],
                    'property_2': numeric_props[j],
                    'correlation': corr_val,
                    'abs_correlation': abs(corr_val)
                })
        
        correlations_df = pd.DataFrame(correlations).sort_values('abs_correlation', ascending=False)
        print("æœ€å¼ºçš„æ€§è´¨é—´ç›¸å…³æ€§ (å‰5ä¸ª):")
        for _, row in correlations_df.head().iterrows():
            print(f"  {row['property_1']} â†” {row['property_2']}: {row['correlation']:.3f}")
        
        return corr_data, correlations_df

    def create_final_mtl_dataset(self, df, property_bundle, dataset_name="final_mtl"):
        """åˆ›å»ºæœ€ç»ˆçš„MTLæ•°æ®é›†"""
        print(f"\n{'='*60}")
        print(f"åˆ›å»ºæœ€ç»ˆMTLæ•°æ®é›†: {dataset_name}")
        print(f"{'='*60}")
        
        # éªŒè¯å±æ€§
        valid_props, invalid_props, _ = self.validate_property_bundle(df, property_bundle, dataset_name)
        
        if invalid_props:
            print(f"\nâš ï¸  å‘ç°æ— æ•ˆå±æ€§: {invalid_props}")
            print(f"ç»§ç»­ä½¿ç”¨æœ‰æ•ˆå±æ€§: {valid_props}")
            property_bundle = valid_props
        
        if len(property_bundle) < 2:
            print("âŒ æœ‰æ•ˆå±æ€§å°‘äº2ä¸ªï¼Œæ— æ³•åˆ›å»ºMTLæ•°æ®é›†")
            return None, None
        
        # åˆ†ææ•°æ®äº¤é›†
        clean_data, intersection_analysis, final_props = self.analyze_data_intersection(df, property_bundle)
        
        if len(clean_data) == 0:
            print("âŒ æ•°æ®äº¤é›†ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºæ•°æ®é›†")
            return None, None
        
        # æ„å»ºæœ€ç»ˆæ•°æ®é›†
        essential_cols = ['jid']
        if 'atoms' in df.columns:
            essential_cols.append('atoms')
        
        final_cols = essential_cols + final_props
        final_dataset = clean_data[final_cols].copy().reset_index(drop=True)
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        print(f"\nğŸ“‹ æœ€ç»ˆæ•°æ®é›†è´¨é‡æŠ¥å‘Š:")
        print(f"   æ ·æœ¬æ•°é‡: {len(final_dataset):,}")
        print(f"   ç‰¹å¾æ•°é‡: {len(final_props)}")
        print(f"   æ€»åˆ—æ•°: {len(final_cols)}")
        
        # å¼‚å¸¸å€¼æ£€æµ‹
        print(f"\nğŸ” å¼‚å¸¸å€¼æ£€æµ‹ (ä½¿ç”¨IQRæ–¹æ³•):")
        for prop in final_props:
            if pd.api.types.is_numeric_dtype(final_dataset[prop]):
                Q1 = final_dataset[prop].quantile(0.25)
                Q3 = final_dataset[prop].quantile(0.75)
                IQR = Q3 - Q1
                outliers = final_dataset[
                    (final_dataset[prop] < (Q1 - 1.5 * IQR)) | 
                    (final_dataset[prop] > (Q3 + 1.5 * IQR))
                ]
                outlier_ratio = len(outliers) / len(final_dataset) * 100
                print(f"   {prop}: {len(outliers)} ä¸ªå¼‚å¸¸å€¼ ({outlier_ratio:.1f}%)")
        
        # ä¿å­˜æ•°æ®é›†å’Œå…ƒæ•°æ®
        dataset_file = os.path.join(self.cache_dir, f'{dataset_name}_dataset.csv')
        final_dataset.to_csv(dataset_file, index=False)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'original_samples': len(df),
            'final_samples': len(final_dataset),
            'retention_rate': len(final_dataset) / len(df) * 100,
            'properties': final_props,
            'dataset_name': dataset_name
        }
        
        metadata_file = os.path.join(self.cache_dir, f'{dataset_name}_metadata.json')
        import json
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"\nğŸ’¾ æ–‡ä»¶å·²ä¿å­˜:")
        print(f"   æ•°æ®é›†: {dataset_file}")
        print(f"   å…ƒæ•°æ®: {metadata_file}")
        
        return final_dataset, metadata

def main():
    """
    ä¸»å‡½æ•°ï¼šå®ç°å®Œæ•´çš„æ•°æ®é€‰æ‹©å’Œå‡†å¤‡æµç¨‹
    è¿™æ˜¯ä¸ºGCPNet-Plusé¡¹ç›®é‡èº«å®šåˆ¶çš„æœ€ç»ˆç‰ˆæœ¬
    """
    print("ğŸš€ GCPNet-Plus æ•°æ®å¤„ç†æµç¨‹å¯åŠ¨")
    print("="*60)
    
    # ===================================================================
    # æ­¥éª¤ 1: æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    # ===================================================================
    handler = JarvisDataHandler()
    raw_df = handler.download_jarvis_data(use_cache=True)
    
    if raw_df is None:
        print("âŒ æ— æ³•è·å–æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
        return
    
    # æ¢ç´¢æ•°æ®ç±»å‹
    handler.explore_column_types(raw_df)
    
    # æ¸…ç†å’Œè½¬æ¢æ•°æ®ç±»å‹
    raw_df = handler.clean_and_convert_data_types(raw_df)
    
    config = OptimizedJarvisConfig()
    
    # ===================================================================
    # æ­¥éª¤ 2: ğŸ† ä½¿ç”¨æœ€ç»ˆæ¨èçš„å±æ€§ç»„åˆ 
    # ===================================================================
    print(f"\nğŸ¯ ä½¿ç”¨æœ€ç»ˆæ¨èçš„MTLå±æ€§ç»„åˆ...")
    
    FINAL_MTL_BUNDLE = config.FINAL_MTL_BUNDLE
    print(f"æœ€ç»ˆç»„åˆåŒ…å« {len(FINAL_MTL_BUNDLE)} ä¸ªæ€§è´¨:")
    for i, prop in enumerate(FINAL_MTL_BUNDLE, 1):
        print(f"  {i}. {prop}")
    
    # éªŒè¯æœ€ç»ˆç»„åˆ
    valid_props, invalid_props, _ = handler.validate_property_bundle(
        raw_df, FINAL_MTL_BUNDLE, "æœ€ç»ˆMTLç»„åˆ"
    )
    
    if len(valid_props) >= 2:  # è‡³å°‘éœ€è¦2ä¸ªæœ‰æ•ˆå±æ€§
        # ç”Ÿæˆç›¸å…³æ€§çŸ©é˜µ
        corr_matrix, corr_stats = handler.plot_enhanced_correlation_matrix(
            raw_df, valid_props, "æœ€ç»ˆMTLç»„åˆ"
        )
        
        # åˆ›å»ºæœ€ç»ˆæ•°æ®é›†
        final_dataset, metadata = handler.create_final_mtl_dataset(
            raw_df, valid_props, "gcpnet_plus_final"
        )
        
        if final_dataset is not None:
            print(f"\nğŸ‰ æœ€ç»ˆæ•°æ®é›†åˆ›å»ºæˆåŠŸ!")
            print(f"   æ•°æ®é›†å½¢çŠ¶: {final_dataset.shape}")
            print(f"   å¯ç”¨äºè®­ç»ƒGCPNet-Plusæ¨¡å‹")
            
            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            print(f"\nğŸ“‹ æ•°æ®é›†é¢„è§ˆ:")
            print(final_dataset.head())
            
            # åªæ˜¾ç¤ºæ•°å€¼åˆ—çš„ç»Ÿè®¡ä¿¡æ¯
            numeric_cols = [col for col in final_dataset.columns if pd.api.types.is_numeric_dtype(final_dataset[col])]
            if numeric_cols:
                print(f"\nğŸ“Š ç›®æ ‡æ€§è´¨ç»Ÿè®¡:")
                print(final_dataset[numeric_cols].describe())
    
    # ===================================================================
    # æ­¥éª¤ 3: å¯¹æ¯”å®éªŒ - åˆ†æå…¶ä»–ç»„åˆ
    # ===================================================================
    print(f"\nğŸ”¬ å¯¹æ¯”åˆ†æï¼šå…¶ä»–å±æ€§ç»„åˆçš„è¡¨ç°")
    print("="*60)
    
    # åˆ†æåŸºç¡€å®Œæ•´ç»„åˆ
    basic_bundle = config.BASIC_COMPLETE_BUNDLE
    print(f"\nğŸ“Œ åˆ†æåŸºç¡€å®Œæ•´ç»„åˆ (ç”¨äºå¯¹æ¯”):")
    valid_basic, _, _ = handler.validate_property_bundle(
        raw_df, basic_bundle, "åŸºç¡€å®Œæ•´ç»„åˆ"
    )
    
    if len(valid_basic) >= 2:
        basic_corr, _ = handler.plot_enhanced_correlation_matrix(
            raw_df, valid_basic, "åŸºç¡€å®Œæ•´ç»„åˆ"
        )
        basic_dataset, _ = handler.create_final_mtl_dataset(
            raw_df, valid_basic, "basic_complete"
        )
        print(f"åŸºç¡€ç»„åˆæœ€ç»ˆæ•°æ®é›†å½¢çŠ¶: {basic_dataset.shape if basic_dataset is not None else 'Failed'}")
    
    # ===================================================================
    # æ­¥éª¤ 4: æ€»ç»“å’Œå»ºè®®
    # ===================================================================
    print(f"\nğŸ å¤„ç†å®Œæˆ - æ€»ç»“æŠ¥å‘Š")
    print("="*60)
    print(f"âœ… æ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ° '{handler.cache_dir}' ç›®å½•")
    print(f"âœ… ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
    print(f"   - gcpnet_plus_final_dataset.csv (ğŸ† æ¨èç”¨äºGCPNet-Plusè®­ç»ƒ)")
    print(f"   - gcpnet_plus_final_metadata.json (æ•°æ®é›†å…ƒä¿¡æ¯)")
    print(f"   - correlation_matrix_*.png (å„ç»„åˆçš„ç›¸å…³æ€§å›¾)")
    print(f"   - column_type_analysis.csv (æ•°æ®ç±»å‹åˆ†æ)")
    print(f"   - data_type_conversion_report.csv (ç±»å‹è½¬æ¢æŠ¥å‘Š)")
    
    print(f"\nğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"   1. æ£€æŸ¥æ•°æ®ç±»å‹è½¬æ¢æŠ¥å‘Šï¼Œç¡®è®¤å…³é”®å±æ€§è½¬æ¢æˆåŠŸ")
    print(f"   2. ä½¿ç”¨ 'gcpnet_plus_final_dataset.csv' è®­ç»ƒæ‚¨çš„æ¨¡å‹")
    print(f"   3. åŸºäºç›¸å…³æ€§åˆ†æç»“æœä¼˜åŒ–æ¨¡å‹æ¶æ„")
    print(f"   4. è€ƒè™‘å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†")

if __name__ == "__main__":
    main()
